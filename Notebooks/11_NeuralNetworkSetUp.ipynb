{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "# from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import datasets as kDatasets\n",
    "from keras import preprocessing as kpreprocess\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process\n",
    "- Important to preprocess as all fetures combined, mean of 0 and std of 1 is usually good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([[-100.1,3240.1], [-200.2, -234.1], [5000.5, 150.1], [6000.6, -125.1], [9000.9, -673.1]])\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "features_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jenn\\Anaconda3\\envs\\everyday\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "# Fully connected with ReLU activation with 10 Features output 16\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(10, )))\n",
    "# Fully connected with ReLU activation\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "# Fully connected with Sigmoid output 1\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "# Compile and add loss function, optimization, and performance metric\n",
    "network.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Layer Activiation:\n",
    "- Binary Classification: sigmoid activation function\n",
    "- Multiclass Classification: k units (# target classes) and softmax activation function\n",
    "- Regression: one unit with no activiation\n",
    "\n",
    "Loss Functions:\n",
    "- Binary Classification: binary_crossentropy\n",
    "- Multiclass Classification: categorical_crossentropy\n",
    "- Regression: Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Opimizer strategy to find parameter functions: stochastic gradient descent (with momentum), adaptive moment estimation\n",
    "- Metrics for evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class\n",
    "- https://en.wikipedia.org/wiki/Loss_functions_for_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_features = 1000\n",
    "# Load data\n",
    "(data_train, target_train), (data_test, target_test) = kDatasets.imdb.load_data(num_words=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert movie review to one hot\n",
    "tokenizer = kpreprocess.text.Tokenizer(num_words=num_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(num_features, )))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jenn\\Anaconda3\\envs\\everyday\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 103us/step - loss: 0.4161 - acc: 0.8134 - val_loss: 0.3347 - val_acc: 0.8586\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 64us/step - loss: 0.3250 - acc: 0.8635 - val_loss: 0.3302 - val_acc: 0.8600\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 63us/step - loss: 0.3160 - acc: 0.8664 - val_loss: 0.3306 - val_acc: 0.8600\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = network.fit(features_train, target_train, epochs=3, verbose=1, batch_size=100,\n",
    "                      validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.33474202746152876, 0.33022729587554933, 0.33062036979198456],\n",
       " 'val_acc': [0.8586399981975555, 0.8599599974155426, 0.8600000011920929],\n",
       " 'loss': [0.41608748406171797, 0.3249572507739067, 0.3160091818571091],\n",
       " 'acc': [0.8133999975919723, 0.8635199983119964, 0.8663999998569488]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiClass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_features = 5000\n",
    "# Load data\n",
    "(data_train, target_train), (data_test, target_test) = kDatasets.reuters.load_data(num_words=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert movie review to one hot\n",
    "tokenizer = kpreprocess.text.Tokenizer(num_words=num_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target vector to create a target matrix\n",
    "tar_train = np_utils.to_categorical(target_train)\n",
    "tar_test = np_utils.to_categorical(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]),\n",
       " [1, 2, 2, 8, 10, 5, 5, 7, 5, 6, 7, 4, 8, 6, 6, 7, 9, 6, 6, 8, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[0][:10], [x for x in data_train[0] if x < 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=100, activation='relu', input_shape=(num_features, )))\n",
    "network.add(layers.Dense(units=100, activation='relu'))\n",
    "network.add(layers.Dense(units=46, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 4s 490us/step - loss: 1.5621 - acc: 0.6653 - val_loss: 1.1439 - val_acc: 0.7395\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 4s 435us/step - loss: 0.8139 - acc: 0.8177 - val_loss: 0.9453 - val_acc: 0.7854\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 4s 398us/step - loss: 0.5242 - acc: 0.8844 - val_loss: 0.8891 - val_acc: 0.7970\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = network.fit(features_train, tar_train, epochs=3, verbose=1, batch_size=100,\n",
    "                      validation_data=(features_test, tar_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = datasets.make_regression(n_samples=10000, n_features=3, n_informative=3, n_targets=1,\n",
    "                                           noise=0.0, random_state=0)\n",
    "# Divide\n",
    "features_train, features_test, target_train, target_test = model_selection.train_test_split(features, target,\n",
    "                                                                                           test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=32, activation='relu', input_shape=(features_train.shape[1], )))\n",
    "network.add(layers.Dense(units=32, activation='relu'))\n",
    "network.add(layers.Dense(units=1))\n",
    "network.compile(loss='mse', optimizer='RMSprop', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/10\n",
      "6700/6700 [==============================] - 1s 81us/step - loss: 17308.3502 - mean_squared_error: 17308.3502 - val_loss: 17665.7711 - val_mean_squared_error: 17665.7711\n",
      "Epoch 2/10\n",
      "6700/6700 [==============================] - 0s 50us/step - loss: 16427.9816 - mean_squared_error: 16427.9816 - val_loss: 16364.2492 - val_mean_squared_error: 16364.2492\n",
      "Epoch 3/10\n",
      "6700/6700 [==============================] - 0s 55us/step - loss: 14751.9711 - mean_squared_error: 14751.9711 - val_loss: 14172.1044 - val_mean_squared_error: 14172.1044\n",
      "Epoch 4/10\n",
      "6700/6700 [==============================] - 0s 25us/step - loss: 12224.2324 - mean_squared_error: 12224.2324 - val_loss: 11115.3894 - val_mean_squared_error: 11115.3894\n",
      "Epoch 5/10\n",
      "6700/6700 [==============================] - 0s 39us/step - loss: 8940.6426 - mean_squared_error: 8940.6426 - val_loss: 7477.5662 - val_mean_squared_error: 7477.5662\n",
      "Epoch 6/10\n",
      "6700/6700 [==============================] - 0s 36us/step - loss: 5418.1875 - mean_squared_error: 5418.1875 - val_loss: 3872.3369 - val_mean_squared_error: 3872.3369\n",
      "Epoch 7/10\n",
      "6700/6700 [==============================] - 0s 39us/step - loss: 2339.9782 - mean_squared_error: 2339.9782 - val_loss: 1269.9520 - val_mean_squared_error: 1269.9520\n",
      "Epoch 8/10\n",
      "6700/6700 [==============================] - 0s 23us/step - loss: 671.7117 - mean_squared_error: 671.7117 - val_loss: 344.6228 - val_mean_squared_error: 344.6228\n",
      "Epoch 9/10\n",
      "6700/6700 [==============================] - 0s 25us/step - loss: 265.6947 - mean_squared_error: 265.6947 - val_loss: 229.3773 - val_mean_squared_error: 229.3773\n",
      "Epoch 10/10\n",
      "6700/6700 [==============================] - 0s 26us/step - loss: 209.5978 - mean_squared_error: 209.5978 - val_loss: 188.3861 - val_mean_squared_error: 188.3861\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = network.fit(features_train, target_train, epochs=10, verbose=1, batch_size=100,\n",
    "                      validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_features = 1000\n",
    "# Load data\n",
    "(data_train, target_train), (data_test, target_test) = kDatasets.imdb.load_data(num_words=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert movie review to one hot\n",
    "tokenizer = kpreprocess.text.Tokenizer(num_words=num_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(num_features, )))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 106us/step - loss: 0.4163 - acc: 0.8131 - val_loss: 0.3343 - val_acc: 0.8585\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.3248 - acc: 0.8636 - val_loss: 0.3293 - val_acc: 0.8603\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.3150 - acc: 0.8664 - val_loss: 0.3295 - val_acc: 0.8599\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = network.fit(features_train, target_train, epochs=3, verbose=1, batch_size=100,\n",
    "                      validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = network.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.24883738],\n",
       "        [0.9984263 ],\n",
       "        [0.50243497],\n",
       "        [0.77616036],\n",
       "        [0.9270196 ],\n",
       "        [0.78844833],\n",
       "        [0.9846772 ],\n",
       "        [0.03175363],\n",
       "        [0.9408301 ],\n",
       "        [0.9189039 ]], dtype=float32),\n",
       " array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_target[:10], target_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
